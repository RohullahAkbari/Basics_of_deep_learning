{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow_path = os.listdir('./datasets/flowers/')\n",
    "flow_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './datasets/flowers/'\n",
    "\n",
    "img_size = 64\n",
    "\n",
    "image = []\n",
    "label = []\n",
    "\n",
    "for i in flow_path:\n",
    "    data_path = path + str(i)\n",
    "    fileName = [i for i in os.listdir(data_path) if i.endswith('jpg')]\n",
    "\n",
    "    # print(fileName)\n",
    "\n",
    "    for f in fileName:\n",
    "        # reading the images\n",
    "        img = cv2.imread(data_path + '/' + f)\n",
    "        # pirnt(img)\n",
    "\n",
    "        # resizing the images\n",
    "        img = cv2.resize(img, (img_size, img_size))\n",
    "\n",
    "        image.append(img)\n",
    "        label.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.545   , 0.5527  , 0.5527  ],\n",
       "         [0.5884  , 0.5845  , 0.5845  ],\n",
       "         [0.6     , 0.596   , 0.596   ],\n",
       "         ...,\n",
       "         [0.655   , 0.6353  , 0.639   ],\n",
       "         [0.596   , 0.604   , 0.604   ],\n",
       "         [0.6     , 0.6     , 0.6     ]],\n",
       "\n",
       "        [[0.5137  , 0.533   , 0.537   ],\n",
       "         [0.565   , 0.569   , 0.5728  ],\n",
       "         [0.678   , 0.655   , 0.655   ],\n",
       "         ...,\n",
       "         [0.639   , 0.6196  , 0.6235  ],\n",
       "         [0.6     , 0.608   , 0.608   ],\n",
       "         [0.5845  , 0.5845  , 0.5845  ]],\n",
       "\n",
       "        [[0.4626  , 0.4902  , 0.4941  ],\n",
       "         [0.537   , 0.549   , 0.541   ],\n",
       "         [0.604   , 0.604   , 0.604   ],\n",
       "         ...,\n",
       "         [0.612   , 0.612   , 0.612   ],\n",
       "         [0.604   , 0.6157  , 0.6157  ],\n",
       "         [0.5566  , 0.5605  , 0.565   ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.0902  , 0.1726  , 0.1608  ],\n",
       "         [0.098   , 0.1804  , 0.1686  ],\n",
       "         [0.1412  , 0.2     , 0.1921  ],\n",
       "         ...,\n",
       "         [0.4785  , 0.4863  , 0.502   ],\n",
       "         [0.4626  , 0.4746  , 0.4902  ],\n",
       "         [0.4587  , 0.4824  , 0.4941  ]],\n",
       "\n",
       "        [[0.098   , 0.1804  , 0.1686  ],\n",
       "         [0.098   , 0.1804  , 0.1686  ],\n",
       "         [0.1451  , 0.2157  , 0.2118  ],\n",
       "         ...,\n",
       "         [0.4941  , 0.498   , 0.5137  ],\n",
       "         [0.4863  , 0.4902  , 0.506   ],\n",
       "         [0.4785  , 0.4824  , 0.498   ]],\n",
       "\n",
       "        [[0.102   , 0.1843  , 0.1726  ],\n",
       "         [0.1059  , 0.1882  , 0.1765  ],\n",
       "         [0.1333  , 0.2157  , 0.2079  ],\n",
       "         ...,\n",
       "         [0.5176  , 0.5215  , 0.537   ],\n",
       "         [0.502   , 0.506   , 0.5215  ],\n",
       "         [0.4902  , 0.4941  , 0.51    ]]],\n",
       "\n",
       "\n",
       "       [[[0.894   , 0.8667  , 0.855   ],\n",
       "         [0.863   , 0.8354  , 0.8237  ],\n",
       "         [0.886   , 0.859   , 0.8433  ],\n",
       "         ...,\n",
       "         [0.0745  , 0.0941  , 0.05884 ],\n",
       "         [0.06665 , 0.0706  , 0.04706 ],\n",
       "         [0.05884 , 0.0353  , 0.04315 ]],\n",
       "\n",
       "        [[0.949   , 0.9136  , 0.906   ],\n",
       "         [0.894   , 0.8745  , 0.8706  ],\n",
       "         [0.886   , 0.855   , 0.8706  ],\n",
       "         ...,\n",
       "         [0.1216  , 0.1333  , 0.102   ],\n",
       "         [0.0745  , 0.06274 , 0.0706  ],\n",
       "         [0.0392  , 0.01569 , 0.02745 ]],\n",
       "\n",
       "        [[0.9175  , 0.902   , 0.894   ],\n",
       "         [0.957   , 0.933   , 0.9136  ],\n",
       "         [0.9453  , 0.9214  , 0.902   ],\n",
       "         ...,\n",
       "         [0.1372  , 0.1177  , 0.1216  ],\n",
       "         [0.08234 , 0.0745  , 0.04706 ],\n",
       "         [0.04706 , 0.02745 , 0.02353 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.718   , 0.698   , 0.706   ],\n",
       "         [0.718   , 0.6943  , 0.698   ],\n",
       "         [0.7373  , 0.7217  , 0.718   ],\n",
       "         ...,\n",
       "         [0.5293  , 0.5605  , 0.596   ],\n",
       "         [0.4587  , 0.498   , 0.545   ],\n",
       "         [0.6353  , 0.6587  , 0.6904  ]],\n",
       "\n",
       "        [[0.706   , 0.6904  , 0.682   ],\n",
       "         [0.6943  , 0.678   , 0.678   ],\n",
       "         [0.8115  , 0.784   , 0.792   ],\n",
       "         ...,\n",
       "         [0.5137  , 0.5527  , 0.5884  ],\n",
       "         [0.4392  , 0.51    , 0.533   ],\n",
       "         [0.643   , 0.6665  , 0.706   ]],\n",
       "\n",
       "        [[0.9097  , 0.886   , 0.863   ],\n",
       "         [0.9414  , 0.9136  , 0.902   ],\n",
       "         [0.937   , 0.9097  , 0.898   ],\n",
       "         ...,\n",
       "         [0.502   , 0.5527  , 0.5845  ],\n",
       "         [0.4314  , 0.537   , 0.5566  ],\n",
       "         [0.6353  , 0.678   , 0.714   ]]],\n",
       "\n",
       "\n",
       "       [[[0.4197  , 0.4666  , 0.4626  ],\n",
       "         [0.3452  , 0.3333  , 0.3765  ],\n",
       "         [0.3843  , 0.3804  , 0.4236  ],\n",
       "         ...,\n",
       "         [0.447   , 0.5293  , 0.4824  ],\n",
       "         [0.6     , 0.6704  , 0.6353  ],\n",
       "         [0.4707  , 0.5527  , 0.533   ]],\n",
       "\n",
       "        [[0.3804  , 0.3608  , 0.3765  ],\n",
       "         [0.2783  , 0.1843  , 0.2588  ],\n",
       "         [0.353   , 0.2942  , 0.353   ],\n",
       "         ...,\n",
       "         [0.51    , 0.5605  , 0.51    ],\n",
       "         [0.4038  , 0.4707  , 0.4353  ],\n",
       "         [0.4314  , 0.502   , 0.4587  ]],\n",
       "\n",
       "        [[0.3687  , 0.298   , 0.349   ],\n",
       "         [0.341   , 0.2705  , 0.3254  ],\n",
       "         [0.341   , 0.302   , 0.3254  ],\n",
       "         ...,\n",
       "         [0.3882  , 0.4119  , 0.408   ],\n",
       "         [0.408   , 0.4746  , 0.4392  ],\n",
       "         [0.3804  , 0.4353  , 0.408   ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.149   , 0.02745 , 0.0863  ],\n",
       "         [0.153   , 0.03137 , 0.0902  ],\n",
       "         [0.1216  , 0.011765, 0.0863  ],\n",
       "         ...,\n",
       "         [0.2744  , 0.06274 , 0.153   ],\n",
       "         [0.3098  , 0.08234 , 0.1804  ],\n",
       "         [0.3215  , 0.08234 , 0.1843  ]],\n",
       "\n",
       "        [[0.1569  , 0.04315 , 0.1059  ],\n",
       "         [0.1412  , 0.02745 , 0.0863  ],\n",
       "         [0.1726  , 0.05884 , 0.0902  ],\n",
       "         ...,\n",
       "         [0.2903  , 0.0706  , 0.1569  ],\n",
       "         [0.298   , 0.0706  , 0.1686  ],\n",
       "         [0.3098  , 0.0706  , 0.1765  ]],\n",
       "\n",
       "        [[0.1451  , 0.03137 , 0.0941  ],\n",
       "         [0.153   , 0.0392  , 0.102   ],\n",
       "         [0.1333  , 0.00784 , 0.098   ],\n",
       "         ...,\n",
       "         [0.302   , 0.0745  , 0.1882  ],\n",
       "         [0.3215  , 0.0863  , 0.204   ],\n",
       "         [0.3333  , 0.0941  , 0.204   ]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.2313  , 0.2744  , 0.2903  ],\n",
       "         [0.2393  , 0.2393  , 0.2157  ],\n",
       "         [0.251   , 0.357   , 0.3647  ],\n",
       "         ...,\n",
       "         [0.6743  , 0.6196  , 0.5176  ],\n",
       "         [0.6     , 0.608   , 0.5176  ],\n",
       "         [0.82    , 0.6587  , 0.447   ]],\n",
       "\n",
       "        [[0.1765  , 0.2     , 0.204   ],\n",
       "         [0.2471  , 0.2666  , 0.2783  ],\n",
       "         [0.3452  , 0.4626  , 0.4824  ],\n",
       "         ...,\n",
       "         [0.3687  , 0.5176  , 0.4626  ],\n",
       "         [0.4353  , 0.5767  , 0.4626  ],\n",
       "         [0.7295  , 0.5767  , 0.4392  ]],\n",
       "\n",
       "        [[0.204   , 0.2157  , 0.2118  ],\n",
       "         [0.2393  , 0.3765  , 0.392   ],\n",
       "         [0.2825  , 0.3333  , 0.306   ],\n",
       "         ...,\n",
       "         [0.3215  , 0.3843  , 0.3333  ],\n",
       "         [0.3843  , 0.4863  , 0.3843  ],\n",
       "         [0.1137  , 0.3372  , 0.251   ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.349   , 0.4902  , 0.5176  ],\n",
       "         [0.3804  , 0.5137  , 0.565   ],\n",
       "         [0.3372  , 0.4785  , 0.51    ],\n",
       "         ...,\n",
       "         [0.1726  , 0.353   , 0.2666  ],\n",
       "         [0.149   , 0.3215  , 0.2196  ],\n",
       "         [0.1647  , 0.3176  , 0.2354  ]],\n",
       "\n",
       "        [[0.443   , 0.5566  , 0.6157  ],\n",
       "         [0.3098  , 0.4314  , 0.4941  ],\n",
       "         [0.2274  , 0.3647  , 0.4038  ],\n",
       "         ...,\n",
       "         [0.0941  , 0.2744  , 0.2     ],\n",
       "         [0.2313  , 0.408   , 0.3176  ],\n",
       "         [0.2354  , 0.4     , 0.3098  ]],\n",
       "\n",
       "        [[0.3254  , 0.4587  , 0.498   ],\n",
       "         [0.3687  , 0.506   , 0.5767  ],\n",
       "         [0.302   , 0.447   , 0.4392  ],\n",
       "         ...,\n",
       "         [0.149   , 0.341   , 0.2627  ],\n",
       "         [0.2354  , 0.4119  , 0.3215  ],\n",
       "         [0.1608  , 0.3215  , 0.255   ]]],\n",
       "\n",
       "\n",
       "       [[[0.604   , 0.596   , 0.6274  ],\n",
       "         [0.9453  , 0.957   , 0.965   ],\n",
       "         [0.788   , 0.7725  , 0.7764  ],\n",
       "         ...,\n",
       "         [0.682   , 0.706   , 0.682   ],\n",
       "         [0.6904  , 0.741   , 0.7373  ],\n",
       "         [0.686   , 0.8115  , 0.796   ]],\n",
       "\n",
       "        [[0.498   , 0.5845  , 0.565   ],\n",
       "         [0.859   , 0.886   , 0.8315  ],\n",
       "         [0.5728  , 0.4548  , 0.4626  ],\n",
       "         ...,\n",
       "         [0.4197  , 0.4902  , 0.5137  ],\n",
       "         [0.1843  , 0.255   , 0.2666  ],\n",
       "         [0.647   , 0.643   , 0.6157  ]],\n",
       "\n",
       "        [[0.7725  , 0.5923  , 0.541   ],\n",
       "         [0.9883  , 0.996   , 1.      ],\n",
       "         [0.6274  , 0.5566  , 0.4392  ],\n",
       "         ...,\n",
       "         [0.8667  , 0.9453  , 0.9453  ],\n",
       "         [0.6626  , 0.8315  , 0.714   ],\n",
       "         [0.788   , 0.8745  , 0.8394  ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.051   , 0.2274  , 0.251   ],\n",
       "         [0.1372  , 0.1686  , 0.1177  ],\n",
       "         [0.0353  , 0.2079  , 0.204   ],\n",
       "         ...,\n",
       "         [0.0549  , 0.1882  , 0.1843  ],\n",
       "         [0.102   , 0.1059  , 0.0784  ],\n",
       "         [0.06665 , 0.1098  , 0.0784  ]],\n",
       "\n",
       "        [[0.0549  , 0.0745  , 0.05884 ],\n",
       "         [0.1726  , 0.255   , 0.2196  ],\n",
       "         [0.04315 , 0.0392  , 0.00392 ],\n",
       "         ...,\n",
       "         [0.0549  , 0.0902  , 0.05884 ],\n",
       "         [0.0549  , 0.098   , 0.05884 ],\n",
       "         [0.051   , 0.1137  , 0.0706  ]],\n",
       "\n",
       "        [[0.1921  , 0.2196  , 0.1372  ],\n",
       "         [0.06665 , 0.0706  , 0.04706 ],\n",
       "         [0.0392  , 0.0549  , 0.03137 ],\n",
       "         ...,\n",
       "         [0.0392  , 0.0863  , 0.0549  ],\n",
       "         [0.0392  , 0.0902  , 0.0392  ],\n",
       "         [0.0196  , 0.1843  , 0.204   ]]],\n",
       "\n",
       "\n",
       "       [[[0.149   , 0.196   , 0.1608  ],\n",
       "         [0.2903  , 0.2942  , 0.2432  ],\n",
       "         [0.6665  , 0.5884  , 0.565   ],\n",
       "         ...,\n",
       "         [0.2     , 0.2118  , 0.1177  ],\n",
       "         [0.541   , 0.4824  , 0.4824  ],\n",
       "         [0.3608  , 0.298   , 0.341   ]],\n",
       "\n",
       "        [[0.1255  , 0.1294  , 0.1804  ],\n",
       "         [0.5923  , 0.5137  , 0.51    ],\n",
       "         [0.306   , 0.3176  , 0.2666  ],\n",
       "         ...,\n",
       "         [0.8354  , 0.741   , 0.8037  ],\n",
       "         [0.5923  , 0.51    , 0.569   ],\n",
       "         [0.549   , 0.4941  , 0.502   ]],\n",
       "\n",
       "        [[0.098   , 0.0941  , 0.102   ],\n",
       "         [0.6626  , 0.5605  , 0.647   ],\n",
       "         [0.04315 , 0.0902  , 0.0549  ],\n",
       "         ...,\n",
       "         [0.4902  , 0.4314  , 0.447   ],\n",
       "         [0.4392  , 0.451   , 0.4119  ],\n",
       "         [0.71    , 0.639   , 0.545   ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.00784 , 0.3293  , 0.6665  ],\n",
       "         [0.02353 , 0.3176  , 0.349   ],\n",
       "         [0.02353 , 0.2235  , 0.1647  ],\n",
       "         ...,\n",
       "         [0.06274 , 0.1647  , 0.1647  ],\n",
       "         [0.05884 , 0.2118  , 0.2627  ],\n",
       "         [0.02745 , 0.2196  , 0.149   ]],\n",
       "\n",
       "        [[0.011765, 0.2     , 0.4158  ],\n",
       "         [0.04706 , 0.1608  , 0.1333  ],\n",
       "         [0.011765, 0.2627  , 0.1098  ],\n",
       "         ...,\n",
       "         [0.03137 , 0.0706  , 0.0902  ],\n",
       "         [0.011765, 0.2354  , 0.1804  ],\n",
       "         [0.0549  , 0.1765  , 0.1372  ]],\n",
       "\n",
       "        [[0.011765, 0.1569  , 0.302   ],\n",
       "         [0.098   , 0.3843  , 0.3293  ],\n",
       "         [0.0353  , 0.3333  , 0.2744  ],\n",
       "         ...,\n",
       "         [0.03137 , 0.2     , 0.1726  ],\n",
       "         [0.01569 , 0.2079  , 0.1843  ],\n",
       "         [0.0196  , 0.2079  , 0.204   ]]]], dtype=float16)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = np.array(image)\n",
    "image = image.astype('float16') / 255\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4317, 64, 64, 3)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 4, 4, 4], dtype=int64)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X = image\n",
    "y = label\n",
    "\n",
    "# change the labels to numeirc values\n",
    "y_label_encoder = LabelEncoder()\n",
    "y = y_label_encoder.fit_transform(y)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oneHotEncodering the y\n",
    "\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "onehotencoder = OneHotEncoder()\n",
    "\n",
    "y = onehotencoder.fit_transform(y)\n",
    "\n",
    "y = y.toarray()\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = shuffle(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3453, 64, 64, 3), (864, 64, 64, 3), (3453, 5), (864, 5))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for processing we should change the shape of images to one dimensional arrays that why we multiply the (64, 64, 3) that it is equal to 12288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(3453, 12288)\n",
    "\n",
    "X_test = X_test.reshape(864, 12288)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3453, 12288), (864, 12288))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .001\n",
    "epoch = 100\n",
    "n_class = 5\n",
    "n_dim = 64\n",
    "\n",
    "x = tf.compat.v1.placeholder(tf.float16, [None, 12288])\n",
    "w = tf.Variable(tf.cast(tf.zeros([n_class, 12288]), tf.float16))\n",
    "b = tf.Variable(tf.cast(tf.zeros([n_class]), tf.float16))\n",
    "y_= tf.compat.v1.placeholder(tf.float16, [None, n_class])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.compat.v1.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred = tf.nn.softmax(tf.matmul(x, tf.transpose(w)) + b) # preceptron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_steps = tf.compat.v1.train.GradientDescentOptimizer(lr).minimize(cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.compat.v1.Session()\n",
    "sess.run(init)\n",
    "cost_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 cost:  1.62\n",
      "epoch:  1 cost:  1.62\n",
      "epoch:  2 cost:  1.619\n",
      "epoch:  3 cost:  1.617\n",
      "epoch:  4 cost:  1.616\n",
      "epoch:  5 cost:  1.615\n",
      "epoch:  6 cost:  1.614\n",
      "epoch:  7 cost:  1.612\n",
      "epoch:  8 cost:  1.61\n",
      "epoch:  9 cost:  1.61\n",
      "epoch:  10 cost:  1.608\n",
      "epoch:  11 cost:  1.607\n",
      "epoch:  12 cost:  1.606\n",
      "epoch:  13 cost:  1.606\n",
      "epoch:  14 cost:  1.604\n",
      "epoch:  15 cost:  1.604\n",
      "epoch:  16 cost:  1.603\n",
      "epoch:  17 cost:  1.602\n",
      "epoch:  18 cost:  1.602\n",
      "epoch:  19 cost:  1.599\n",
      "epoch:  20 cost:  1.599\n",
      "epoch:  21 cost:  1.599\n",
      "epoch:  22 cost:  1.599\n",
      "epoch:  23 cost:  1.598\n",
      "epoch:  24 cost:  1.597\n",
      "epoch:  25 cost:  1.597\n",
      "epoch:  26 cost:  1.597\n",
      "epoch:  27 cost:  1.597\n",
      "epoch:  28 cost:  1.597\n",
      "epoch:  29 cost:  1.597\n",
      "epoch:  30 cost:  1.596\n",
      "epoch:  31 cost:  1.596\n",
      "epoch:  32 cost:  1.595\n",
      "epoch:  33 cost:  1.595\n",
      "epoch:  34 cost:  1.595\n",
      "epoch:  35 cost:  1.595\n",
      "epoch:  36 cost:  1.595\n",
      "epoch:  37 cost:  1.595\n",
      "epoch:  38 cost:  1.595\n",
      "epoch:  39 cost:  1.595\n",
      "epoch:  40 cost:  1.594\n",
      "epoch:  41 cost:  1.594\n",
      "epoch:  42 cost:  1.594\n",
      "epoch:  43 cost:  1.594\n",
      "epoch:  44 cost:  1.595\n",
      "epoch:  45 cost:  1.595\n",
      "epoch:  46 cost:  1.594\n",
      "epoch:  47 cost:  1.594\n",
      "epoch:  48 cost:  1.594\n",
      "epoch:  49 cost:  1.592\n",
      "epoch:  50 cost:  1.592\n",
      "epoch:  51 cost:  1.592\n",
      "epoch:  52 cost:  1.592\n",
      "epoch:  53 cost:  1.592\n",
      "epoch:  54 cost:  1.592\n",
      "epoch:  55 cost:  1.592\n",
      "epoch:  56 cost:  1.591\n",
      "epoch:  57 cost:  1.59\n",
      "epoch:  58 cost:  1.591\n",
      "epoch:  59 cost:  1.59\n",
      "epoch:  60 cost:  1.59\n",
      "epoch:  61 cost:  1.59\n",
      "epoch:  62 cost:  1.59\n",
      "epoch:  63 cost:  1.59\n",
      "epoch:  64 cost:  1.59\n",
      "epoch:  65 cost:  1.59\n",
      "epoch:  66 cost:  1.59\n",
      "epoch:  67 cost:  1.59\n",
      "epoch:  68 cost:  1.59\n",
      "epoch:  69 cost:  1.59\n",
      "epoch:  70 cost:  1.59\n",
      "epoch:  71 cost:  1.59\n",
      "epoch:  72 cost:  1.59\n",
      "epoch:  73 cost:  1.59\n",
      "epoch:  74 cost:  1.59\n",
      "epoch:  75 cost:  1.59\n",
      "epoch:  76 cost:  1.589\n",
      "epoch:  77 cost:  1.589\n",
      "epoch:  78 cost:  1.589\n",
      "epoch:  79 cost:  1.589\n",
      "epoch:  80 cost:  1.589\n",
      "epoch:  81 cost:  1.589\n",
      "epoch:  82 cost:  1.589\n",
      "epoch:  83 cost:  1.589\n",
      "epoch:  84 cost:  1.589\n",
      "epoch:  85 cost:  1.589\n",
      "epoch:  86 cost:  1.589\n",
      "epoch:  87 cost:  1.589\n",
      "epoch:  88 cost:  1.589\n",
      "epoch:  89 cost:  1.588\n",
      "epoch:  90 cost:  1.588\n",
      "epoch:  91 cost:  1.588\n",
      "epoch:  92 cost:  1.588\n",
      "epoch:  93 cost:  1.588\n",
      "epoch:  94 cost:  1.588\n",
      "epoch:  95 cost:  1.588\n",
      "epoch:  96 cost:  1.588\n",
      "epoch:  97 cost:  1.586\n",
      "epoch:  98 cost:  1.586\n",
      "epoch:  99 cost:  1.586\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    sess.run(training_steps, feed_dict={x:X_train, y_:y_train})\n",
    "    cost = sess.run(cost_function, feed_dict={x:X_train, y_:y_train})\n",
    "\n",
    "    cost_history = np.append(cost_history, cost)\n",
    "\n",
    "    print('epoch: ', i , 'cost: ', cost)\n",
    "\n",
    "y_pred = sess.run(pred, feed_dict={x:X_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
